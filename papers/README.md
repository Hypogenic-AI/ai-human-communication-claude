# Downloaded Papers

This directory contains 11 papers relevant to AI-to-Human Communication research.

---

## Paper List

### 1. Progressive Disclosure: Designing for Effective Transparency
- **File**: `1811.02164_Progressive_Disclosure_Transparency.pdf`
- **Authors**: Aaron Springer, Steve Whittaker
- **Year**: 2018
- **arXiv**: [1811.02164](https://arxiv.org/abs/1811.02164)
- **Why relevant**: Foundational work on progressive disclosure in AI systems. Shows that simpler initial feedback helps users build mental models before receiving detailed explanations. Directly applicable to how we present AI research output.

---

### 2. Task Supportive and Personalized Human-Large Language Model Interaction
- **File**: `2402.06170_Task_Supportive_Human_LLM.pdf`
- **Authors**: Ben Wang, Jiqun Liu, Jamshed Karimnazarov, Nicolas Thompson
- **Year**: 2024
- **arXiv**: [2402.06170](https://arxiv.org/abs/2402.06170)
- **Conference**: ACM CHIIR 2024
- **Why relevant**: Demonstrates practical supportive functions (perception articulation, prompt suggestion, conversation explanation) that reduce cognitive load in human-LLM interaction.

---

### 3. A Comprehensive Survey on Automatic Text Summarization with LLM Methods
- **File**: `2403.02901_Text_Summarization_LLM_Survey.pdf`
- **Authors**: Yang Zhang, Hanlei Jin, Dan Meng, Jun Wang, Jinghua Tan
- **Year**: 2024
- **arXiv**: [2403.02901](https://arxiv.org/abs/2403.02901)
- **Why relevant**: Comprehensive survey covering extractive, abstractive, and LLM-based summarization. Provides foundation for understanding state-of-the-art in condensing information.

---

### 4. A Comparative Study of Quality Evaluation Methods for Text Summarization
- **File**: `2407.00747_Quality_Eval_Summarization.pdf`
- **Authors**: Huyen Nguyen, Haihua Chen, Lavanya Pobbathi, Junhua Ding
- **Year**: 2024
- **arXiv**: [2407.00747](https://arxiv.org/abs/2407.00747)
- **Why relevant**: Compares 8 automatic metrics with human evaluation. Shows LLM-based evaluation aligns better with human judgment than ROUGE/BERTScore. Critical for experiment design.

---

### 5. CogErgLLM: Cognitive Ergonomics in LLM System Design
- **File**: `2407.02885_CogErgLLM_Cognitive_Ergonomics.pdf`
- **Authors**: Azmine Toushik Wasi, Mst Rafia Islam
- **Year**: 2024
- **arXiv**: [2407.02885](https://arxiv.org/abs/2407.02885)
- **Why relevant**: Framework for integrating cognitive ergonomics into LLM design. Covers efficiency, attention support, learning facilitation, and decision-making aid principles.

---

### 6. Agentic Information Retrieval
- **File**: `2410.09713_Agentic_Information_Retrieval.pdf`
- **Authors**: Weinan Zhang, Junwei Liao, Ning Li, Kounianhua Du, Jianghao Lin
- **Year**: 2024
- **arXiv**: [2410.09713](https://arxiv.org/abs/2410.09713)
- **Why relevant**: Introduces "agentic IR" paradigm - dynamic, context-dependent information states managed by LLM agents. Relevant for how AI research agents present findings adaptively.

---

### 7. Learning to Summarize from LLM-generated Feedback (FeedSum)
- **File**: `2410.13116_FeedSum_LLM_Summary_Feedback.pdf`
- **Authors**: Hwanjun Song, Taewon Yun, Yuho Lee, Jihwan Oh, Gihun Lee, Jason Cai, Hang Su
- **Year**: 2024
- **arXiv**: [2410.13116](https://arxiv.org/abs/2410.13116)
- **Why relevant**: Introduces FeedSum dataset and shows how multi-dimensional feedback (faithfulness, completeness, conciseness) improves summarization. Directly applicable to training human-preferred communication.

---

### 8. CARE: Collaborative AI Research Environment
- **File**: `2410.24032_CARE_Collaborative_Assistant.pdf`
- **Authors**: John P. Lalor, Xiaoyu (Rosie) Yang, Huasong Leng, Zikui Cai, et al.
- **Year**: 2024
- **arXiv**: [2410.24032](https://arxiv.org/abs/2410.24032)
- **Why relevant**: System for collaborative human-AI research. Model for presenting AI research outputs in ways that support human comprehension.

---

### 9. Reference-Free Evaluation Metrics for Summarization
- **File**: `2501.12011_Reference_Free_Eval_Metrics.pdf`
- **Year**: 2025
- **arXiv**: [2501.12011](https://arxiv.org/abs/2501.12011)
- **Why relevant**: Methods for evaluating summary quality without reference summaries. Important for real-world deployment where gold standards unavailable.

---

### 10. Cognitive Load in Streaming LLM Output
- **File**: `2504.17999_Cognitive_Load_Streaming_LLM.pdf`
- **Year**: 2025
- **arXiv**: [2504.17999](https://arxiv.org/abs/2504.17999)
- **Why relevant**: Studies cognitive load when consuming streaming LLM output. Pacing and chunking influence comprehension - critical for AI-to-human communication design.

---

### 11. Generative Interfaces for LLM Systems
- **File**: `2508.19227_Generative_Interfaces_LLM.pdf`
- **Year**: 2025
- **arXiv**: [2508.19227](https://arxiv.org/abs/2508.19227)
- **Why relevant**: Design patterns for LLM interfaces that improve usability. Progressive revelation patterns and visual hierarchy for complex information.

---

## Summary by Topic

### Progressive Disclosure & Transparency
- Paper 1: Progressive Disclosure (foundational)
- Paper 11: Generative Interfaces (design patterns)

### Cognitive Load & Ergonomics
- Paper 2: Task Supportive Human-LLM Interaction
- Paper 5: CogErgLLM Framework
- Paper 10: Cognitive Load in Streaming

### Summarization Methods
- Paper 3: Text Summarization Survey
- Paper 7: FeedSum (multi-dimensional feedback)

### Evaluation Methods
- Paper 4: Quality Evaluation Comparison
- Paper 9: Reference-Free Evaluation

### Information Retrieval & Collaboration
- Paper 6: Agentic Information Retrieval
- Paper 8: CARE Collaborative Environment
